{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding Unsupervised Learning\n",
    "A wide variety of clustering algorithms are available in scikit-learn. The hard part is interpreting the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"./genres_data.json\", \"r\") as infile:\n",
    "    genres_data = json.load(infile)\n",
    "print(len(genres_data))\n",
    "print(genres_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Preprocess Data:\n",
    "data_matrix = []\n",
    "for datum in genres_data:\n",
    "    fields = [\"total_books_bought\", \"literary_fiction_fraction\", \"mystery_novels_fraction\",\n",
    "              \"programming_references_fraction\", \"popular_science_fraction\", \"science_fiction_fraction\", \"fantasy_fraction\"]\n",
    "    data_vector = [datum[field] for field in fields]\n",
    "    data_matrix.append(data_vector)\n",
    "\n",
    "data_matrix = np.array(data_matrix)\n",
    "\n",
    "print(data_matrix[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KMeans(n_clusters=2)\n",
    "model.fit(data_matrix)\n",
    "print(model.cluster_centers_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_nums = list(range(2, 10 + 1))\n",
    "scores = []\n",
    "for num in cluster_nums:\n",
    "    model = KMeans(n_clusters=num)\n",
    "    model.fit(data_matrix)\n",
    "    scores.append(model.score(data_matrix))\n",
    "plt.title(\"Scores versus n_clusters\")\n",
    "plt.plot(cluster_nums, scores, 'b*-')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KMeans(n_clusters=5)\n",
    "model.fit(data_matrix)\n",
    "print(model.cluster_centers_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Expectation Maximization - Mixture of Gaussians"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def best_model(scores, models, func=min):\n",
    "    best_model = models[0]\n",
    "    best_score = scores[0]\n",
    "    for i in range(1, len(scores)):\n",
    "        if scores[i] < best_score:\n",
    "            best_score = scores[i]\n",
    "            best_model = models[i]\n",
    "    return best_score, best_model\n",
    "\n",
    "def try_models(n_clusters, n_trials, data):\n",
    "    scores = []\n",
    "    models = []\n",
    "    for i in range(n_trials):\n",
    "        model = GaussianMixture(n_components=n_clusters)\n",
    "        model.fit(data)\n",
    "        models.append(model)\n",
    "        scores.append(model.bic(data_matrix)) # Bayesian Information Criterion measures usefulness (lower is better)\n",
    "    return scores, models\n",
    "\n",
    "cluster_nums = list(range(2, 20 + 1))\n",
    "best_scores = []\n",
    "best_models = []\n",
    "for num in cluster_nums:\n",
    "    scores, models = try_models(num, 5, data_matrix)\n",
    "    min_score, model = best_model(scores, models)\n",
    "    best_models.append(model)\n",
    "    best_scores.append(min_score)\n",
    "plt.title(\"Scores versus n_clusters\")\n",
    "plt.plot(cluster_nums, best_scores, 'b*-')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "float_formatter = lambda x: \"%.2f\" % x # Two decimal places is probably enough here.\n",
    "np.set_printoptions(formatter={'float_kind':float_formatter})\n",
    "print(best_models[9].means_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note:\n",
    "The n_components in a Gaussian Mixture Model is the number of Gaussians. This has a different interpretation than the number of clusters in, e.g., k-means. In particular, multiple Gaussians may be representing very similar/overlapping data in the Gaussian Mixture Model, whereas in other clustering algorithms, the data are often well-separated. Gaussian Mixture Models are considered a \"Generative Model\" of data, meaning that they provide a model from which the data could plausibly have been generated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings with t-SNE\n",
    "(\"t-distributed Stochastic Neighbor Embedding\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import scale\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Whiten Data and run PCA\n",
    "whitened_data = scale(data_matrix)\n",
    "\n",
    "pca = PCA()\n",
    "pca_data = pca.fit_transform(whitened_data)\n",
    "\n",
    "float_formatter = lambda x: \"%.7f\" % x # Two decimal places is probably enough here.\n",
    "np.set_printoptions(formatter={'float_kind':float_formatter})\n",
    "print(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since the last dimension appears to be redundant, we can safely eliminate it.\n",
    "pca = PCA(n_components=6)\n",
    "pca_data = pca.fit_transform(whitened_data)\n",
    "print(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "embedding = TSNE(n_components=2) # Here, the n_components is the number of dimensions, not number of clusters.\n",
    "embedded_data = embedding.fit_transform(pca_data) # Creates 2-d embeddings of the original data.\n",
    "print(\"Data Shape\", embedded_data.shape)\n",
    "\n",
    "# We can now plot the data:\n",
    "plt.title(\"t-SNE embeddings of our data\")\n",
    "plt.scatter(embedded_data[:,0], embedded_data[:,1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can cluster in the embedding space\n",
    "model = KMeans(n_clusters=6)\n",
    "labels = model.fit_predict(embedded_data)\n",
    "\n",
    "plt.title(\"Embedding Space Clusters\")\n",
    "plt.scatter(embedded_data[:,0], embedded_data[:,1], c=labels)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hierarchical Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that this is from scipy, not sklearn!\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "connections = linkage(data_matrix, 'ward') # Links clusters so that variance of the group is minimized\n",
    "\n",
    "# calculate full dendrogram\n",
    "plt.figure(figsize=(25, 10))\n",
    "plt.title('Hierarchical Clustering Dendrogram')\n",
    "plt.xlabel('sample index')\n",
    "plt.ylabel('distance')\n",
    "dendrogram(\n",
    "    connections,\n",
    "    #leaf_rotation=90.,  # rotates the x axis labels\n",
    "    #leaf_font_size=8.,  # font size for the x axis labels\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "model = AgglomerativeClustering(n_clusters=2)\n",
    "clusters = model.fit_predict(data_matrix)\n",
    "\n",
    "plt.scatter(embedded_data[:,0], embedded_data[:,1], c=clusters, cmap='prism')  # plot points with cluster dependent colors\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
